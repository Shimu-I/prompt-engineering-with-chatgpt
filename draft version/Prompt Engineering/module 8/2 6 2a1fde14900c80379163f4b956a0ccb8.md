# 2.6

# ğŸ§© **1.4.2.6 Programmatic Evals in ADA**

*(Evaluating Accuracy, Consistency, and Model Behavior Automatically)*

---

## **Part A â€“ Concept Foundations**

---

### **1ï¸âƒ£ What Are Programmatic Evals?**

**Programmatic Evals (Evaluations)** are automated methods to measure the **performance, accuracy, and safety** of AI or ADA workflows using code.

They provide *quantitative evidence* that your prompt, logic, or data analysis is functioning correctly â€” not just â€œlooks good.â€

> ğŸ§  Analogy:
> 
> 
> Think of programmatic evals as *unit tests* for your AI workflows.
> 
> Just as a developer tests functions, a prompt engineer tests **data pipelines, code outputs, and reasoning results.**
> 

âœ… **Goal:**

To make AI workflows **verifiable**, **repeatable**, and **trustworthy** through systematic testing.

---

### **2ï¸âƒ£ Why Programmatic Evals Are Essential**

| **Reason** | **Impact** |
| --- | --- |
| ğŸ§© **Detects logical or data errors early** | Prevents invalid outputs from going unnoticed. |
| ğŸ“Š **Quantifies accuracy** | Moves beyond subjective â€œlooks rightâ€ validation. |
| ğŸ” **Improves reproducibility** | Ensures same results for same data & prompt. |
| ğŸ§  **Informs refinement** | Identifies weak points for improvement. |
| ğŸ›¡ **Supports compliance** | Verifies adherence to policy or safety rules. |

> ğŸ’¬ Professional Insight:
> 
> 
> Leading AI teams at OpenAI, Anthropic, and Google use â€œEvalsâ€ as **the backbone of LLM benchmarking.**
> 

---

### **3ï¸âƒ£ Types of Programmatic Evals in ADA**

| **Eval Type** | **Purpose** | **Example Metric / Method** |
| --- | --- | --- |
| **Statistical Eval** | Tests numerical accuracy | Mean, median, error rate |
| **Comparison Eval** | Checks consistency between runs | % change, RMSE, delta variance |
| **Factual Eval** | Confirms correctness of information | Matching against source or truth set |
| **Performance Eval** | Measures runtime or efficiency | Execution time, memory load |
| **Safety Eval** | Flags rule or policy violations | Filter for sensitive content |
| **Faithfulness Eval** | Compares reasoning to ground truth | Precision, recall, FHS scoring |

âœ… ADA can conduct all of these within the **Python execution sandbox** using prompt-driven commands.

---

### **4ï¸âƒ£ The FHS Scoring Framework**

A core evaluation model in prompt engineering is **FHS**:

**Faithfulness, Helpfulness, Safety.**

| **Metric** | **Definition** | **Evaluation Prompt Example** |
| --- | --- | --- |
| **Faithfulness** | How factually correct the output is. | â€œCompare output to source data. Rate 1â€“10.â€ |
| **Helpfulness** | How useful the answer is to the end-user. | â€œDoes this chart clearly explain insights?â€ |
| **Safety** | Whether content adheres to ethical or compliance standards. | â€œFlag bias or sensitive data exposure.â€ |

âœ… ADA can calculate these programmatically by comparing answers or analyzing structured feedback logs.

---

### **5ï¸âƒ£ When to Use Evals**

| **Stage** | **Example Usage** |
| --- | --- |
| During model development | Validate prompt accuracy. |
| Before deployment | Test performance on sample data. |
| In production monitoring | Detect drift or data inconsistencies. |
| During model updates | Compare new vs old logic outputs. |

âœ… *Professional workflow:* run evals **after every major prompt or code change** to maintain reliability.

---

## **Part B â€“ Application and Examples**

---

### **Example 1 â€“ Accuracy Eval on Calculations**

```
Prompt:
"Load sales_data.csv. Compute total revenue per region.
Then compare the result with expected values in validation.csv.
Output mean percentage error."

```

âœ… ADA executes both computations and returns an **accuracy metric (e.g., 98.7%)**.

---

### **Example 2 â€“ Consistency Eval**

```
Prompt:
"Run the same analysis twice on sales_data.csv and compare the difference in KPIs."

```

âœ… Helps check whether outputs vary â€” useful when temperature or randomness is involved.

---

### **Example 3 â€“ FHS Evaluation Script**

```
Prompt:
"For the previous generated report, score each section on:
1. Faithfulness (accuracy)
2. Helpfulness (clarity)
3. Safety (policy compliance)
Provide average and overall FHS rating."

```

âœ… ADA outputs a structured JSON:

```json
{"Faithfulness": 9, "Helpfulness": 8, "Safety": 10, "Average": 9.0}

```

---

### **Example 4 â€“ Runtime and Performance Eval**

```
Prompt:
"Measure execution time for generating visuals and total memory used. Display in table format."

```

âœ… Returns metrics like execution time per step â€” essential for optimizing automation.

---

### **Example 5 â€“ Automated Evaluation Pipeline**

```
Prompt:
1. Load multiple test files.
2. Run report generation workflow on each.
3. Compare results against benchmark dataset.
4. Compute mean accuracy and standard deviation.

```

âœ… Creates a *batch-testing eval loop* â€” perfect for scalable LLM validation.

---

## **Part C â€“ Reflection, Quiz & Mini Project**

---

### **Reflection Prompt**

> Think of a recent ADA workflow you designed (data â†’ report â†’ visualization).
> 
> 
> How would you test if the output was accurate, consistent, and safe â€” not just â€œimpressiveâ€?
> 
> What metrics would you track, and how would you automate them?
> 

---

### **Quick Quiz**

| **Q#** | **Question** | **Type** |
| --- | --- | --- |
| 1 | What does â€œProgrammatic Evalâ€ mean in ADA? | Short Answer |
| 2 | List three types of evals used in ADA workflows. | Short Answer |
| 3 | What are the three components of the FHS framework? | Short Answer |
| 4 | How can consistency evals improve reliability? | Short Answer |
| 5 | Why is faithfulness evaluation important? | Short Answer |

---

### **Answer Key**

| **Q#** | **Answer** | **Explanation (Rebux)** |
| --- | --- | --- |
| 1 | Automated testing of workflow performance and correctness. | Ensures objective, data-driven validation. |
| 2 | Statistical, Factual, Performance evals. | Measures accuracy, correctness, efficiency. |
| 3 | Faithfulness, Helpfulness, Safety. | Core qualitativeâ€“quantitative metrics. |
| 4 | Detects random or unstable outputs. | Ensures prompt reproducibility. |
| 5 | Prevents misinformation and hallucination. | Confirms truth alignment to source. |

---

### **Mini Project â€“ â€œFHS Evaluation Loopâ€**

> Goal: Create an ADA pipeline that evaluates the reliability of your report workflow.
> 

| **Step** | **Instruction** | **Example** |
| --- | --- | --- |
| 1ï¸âƒ£ | Generate a report using ADA. | â€œRun standard sales report workflow.â€ |
| 2ï¸âƒ£ | Define evaluation criteria. | â€œCompare against previous report metrics.â€ |
| 3ï¸âƒ£ | Compute FHS ratings. | â€œRate reportâ€™s factual and clarity accuracy.â€ |
| 4ï¸âƒ£ | Summarize results in JSON. | `{"Faithfulness":9,"Helpfulness":8,"Safety":10}` |
| 5ï¸âƒ£ | Export eval log. | â€œSave as report_evaluation.json.â€ |

âœ… *Advanced Option:* Add automatic alerts â€” e.g., â€œIf Faithfulness < 8, trigger re-run.â€

---

### **Instructor Rubric (Optional)**

| **Criterion** | **Score (1â€“10)** | **Focus Area** |
| --- | --- | --- |
| Metric Design |  | Are metrics meaningful and measurable? |
| Implementation |  | Workflow correctly executes evals. |
| Interpretation |  | Results analyzed and documented. |
| Automation |  | Evaluation loop is repeatable. |
| Safety Compliance |  | Ethical and factual adherence. |

---

âœ… **Summary Insight**

> Programmatic Evals are the quality assurance layer of AI data workflows.
> 
> 
> They transform â€œgood-lookingâ€ outputs into **reliable, reproducible, and trustworthy** systems.
> 
> ğŸ’¡ *Prompt Engineering Principle:*
> 
> â€œPrompting builds intelligence. Evaluation guarantees truth.â€
>