# 2.9

# 🧩 **1.4.2.9 Exam Section — Applied Prompt + Reflection**

---

## **Part A – Concept Foundations**

---

### **1️⃣ Purpose of the ADA Exam Section**

This exam tests your ability to:

- **Combine technical and prompt-engineering skills**
- **Automate complete workflows** (data → visualization → report)
- **Evaluate and document** ADA results with reproducibility
- **Reflect on your design choices** and their implications

> 🧠 Analogy:
> 
> 
> Think of this as your **“Capstone Sprint”** — proving that you can transform raw data into insight, automation, and validation with *no manual coding beyond prompts.*
> 

✅ **Goal:**

To demonstrate mastery of **ADA pipelines** as an *AI-integrated data system.*

---

### **2️⃣ Core Competency Areas**

| **Competency** | **What It Tests** |
| --- | --- |
| **Prompt Clarity** | How well you instruct ADA to analyze and visualize data |
| **Data Handling** | Cleaning, merging, transforming diverse file types |
| **Visualization** | Ability to produce charts and dashboards |
| **Automation** | Workflow sequencing and batch processing |
| **Reproducibility** | Seeds, logs, and consistent outputs |
| **Evaluation** | Quantitative & qualitative analysis of results |
| **Reflection** | Insight into design reasoning and optimization |

✅ All six pillars of professional ADA engineering are assessed.

---

## **Part B – Applied Challenge (Exam Project)**

---

### **🎯 Project Title:**

**“Building a Reproducible ADA Pipeline for Sales Insights”**

---

### **Scenario Overview**

Your company uploads **monthly regional sales reports** in `.csv` and `.xlsx` formats.

You are tasked to build an ADA workflow that:

1. Ingests and cleans all files
2. Merges datasets into a single unified table
3. Generates analytics and visualizations
4. Exports a professional report
5. Logs all steps for reproducibility and evaluation

---

### **Exam Tasks**

### **Task 1 – Data Loading and Validation**

**Prompt Example:**

```
Upload all monthly_sales_*.csv and regional_summary.xlsx.
Inspect for missing values, inconsistent column names, and data type mismatches.
Standardize column formats (e.g., 'Region', 'Revenue', 'Profit').

```

✅ **Evaluation Criteria:**

- Proper ingestion of multiple file formats
- Detection of missing or inconsistent data
- Clear data normalization strategy

---

### **Task 2 – Data Analysis and Visualization**

**Prompt Example:**

```
Compute total revenue and profit per region.
Create a bar chart comparing monthly sales by region.
Plot a trend line for total revenue over time.

```

✅ **Evaluation Criteria:**

- Accuracy of computed KPIs
- Clarity of visualizations
- Correct labeling and legends

---

### **Task 3 – Automation Workflow Design**

**Prompt Example:**

```
Build an automation that:
1. Merges all new files uploaded in a folder.
2. Cleans and summarizes key metrics.
3. Generates charts.
4. Exports a PDF titled 'Monthly Performance Report [Month-Year]'.

```

✅ **Evaluation Criteria:**

- Logical sequencing of operations
- Batch reliability and fault tolerance
- Proper file naming and export control

---

### **Task 4 – Reproducibility Logging**

**Prompt Example:**

```
Set random seed = 42.
Record each step’s timestamp, file names, and environment metadata.
Export run_log.json with summary of input/output files.

```

✅ **Evaluation Criteria:**

- Deterministic results across reruns
- Presence of metadata (library versions, seed, etc.)
- Clean, readable JSON log structure

---

### **Task 5 – Programmatic Evaluation**

**Prompt Example:**

```
Compare current month’s report to previous month’s metrics.
Compute Faithfulness, Helpfulness, and Safety (FHS) scores for insights.
Export evaluation as evaluation_metrics.json.

```

✅ **Evaluation Criteria:**

- Correct comparative analysis
- Valid metric computation
- Structured eval output file

---

### **Task 6 – Final Reflection Prompt**

**Prompt Example:**

```
Reflect on:
1. How reproducibility improved workflow stability
2. Which automation step saved the most time
3. How FHS evaluation ensures trustworthy analytics
Summarize insights in 200 words.

```

✅ **Evaluation Criteria:**

- Depth of understanding
- Practical reflection and reasoning
- Use of professional tone and structured argument

---

## **Part C – Self-Assessment Quiz & Answer Key**

---

### **Quick Quiz (10 Questions)**

| **Q#** | **Question** | **Type** |
| --- | --- | --- |
| 1 | What are the three pillars of ADA reproducibility? | Short Answer |
| 2 | How can ADA automate weekly reports? | Short Answer |
| 3 | What is the FHS framework used for? | Short Answer |
| 4 | How does setting a random seed improve reliability? | Short Answer |
| 5 | Name two output formats ADA can generate. | Short Answer |
| 6 | Why is logging necessary in batch workflows? | Short Answer |
| 7 | What does “Programmatic Eval” measure? | Short Answer |
| 8 | Give one example of a Chain Pattern in automation. | Short Answer |
| 9 | How do ADA visualizations support storytelling? | Short Answer |
| 10 | Why is reproducibility critical for ethical AI use? | Short Answer |

---

### **Answer Key**

| **Q#** | **Answer** | **Explanation (Rebux)** |
| --- | --- | --- |
| 1 | Seeds, Notebooks, Logs. | Foundational reproducibility elements. |
| 2 | Through scheduled or batch prompts. | Automates data → report pipeline. |
| 3 | Evaluates Faithfulness, Helpfulness, Safety. | Measures reliability and ethics. |
| 4 | Removes randomness, ensuring same results. | Deterministic workflow design. |
| 5 | PDF, CSV, JSON, Markdown, HTML. | Common ADA export formats. |
| 6 | Records data lineage and errors. | Supports audits and debugging. |
| 7 | Accuracy, performance, and safety of workflow. | Quantitative and qualitative eval. |
| 8 | Clean → Analyze → Visualize → Export. | Sequential workflow execution. |
| 9 | Translates data into intuitive visuals. | Enhances comprehension and decision-making. |
| 10 | Enables accountability and trust. | Prevents opaque AI results. |

---

## **Part D – Instructor Rubric (Comprehensive Evaluation)**

| **Category** | **Max Points** | **Description** |
| --- | --- | --- |
| Data Handling & Cleaning | 10 | Accuracy and quality of preprocessing |
| Visualization & Reporting | 10 | Effectiveness and clarity of visuals |
| Workflow Automation | 10 | Logical and efficient sequence design |
| Reproducibility | 10 | Deterministic outputs and detailed logs |
| Evaluation & Reflection | 10 | Insightfulness and completeness of reflection |
| Documentation Quality | 10 | Readable, consistent, and LMS-ready submission |

✅ **Total: 60 Points**

---

## **Part E – Final Reflection Summary**

> "The true test of ADA mastery lies not in producing insights, but in producing them reliably, ethically, and reproducibly."
> 
> 
> Through this exam, you’ve proven that you can:
> 
> - Engineer complete ADA workflows
> - Automate and evaluate your outputs
> - Maintain transparency through logs and seeds
> - Communicate results like a data scientist and educator

---

✅ **Module 8 Master Insight:**

> ADA transforms prompt engineers into applied AI analysts — capable of designing, running, and auditing data pipelines through language alone.
> 
> 
> 💡 *Prompt Engineering Principle:*
> 
> “Automation builds scale; evaluation builds trust; reproducibility builds legacy.”
>