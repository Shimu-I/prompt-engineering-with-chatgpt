# 2.5

# 🧩 **1.3.2.5 Prompt Evaluation Metrics**

---

### **Part A – Concept Foundations**

### **1️⃣ Why We Need Prompt Evaluation**

Prompt Engineering isn’t only about *getting outputs* — it’s about *measuring* how good those outputs are.

Without objective evaluation, we can’t improve prompts, compare approaches, or ensure reliability.

> 🧠 Analogy:
> 
> 
> Think of your prompt like a scientific experiment.
> 
> You wouldn’t just look at the result — you’d measure *accuracy, efficiency, and reproducibility.*
> 
> Evaluation metrics let you quantify prompt performance just like engineers measure code efficiency or researchers measure precision.
> 

✅ The goal: make prompt design **scientific**, not **subjective**.

---

### **2️⃣ What Are Prompt Evaluation Metrics?**

Prompt Evaluation Metrics are **quantitative or qualitative indicators** used to judge whether a prompt produces:

- Relevant
- Accurate
- Faithful
- Efficient
- Safe

responses.

They are the **rubrics** of prompt design.

---

### **3️⃣ The Core Evaluation Framework: FHC Model**

The **FHC Model** — Faithfulness, Helpfulness, and Cost — is the most common way to evaluate prompt quality.

| **Metric** | **Definition** | **Ideal Measurement** |
| --- | --- | --- |
| **Faithfulness (F)** | How accurate and grounded the answer is to the prompt and data. | Factually correct, avoids hallucination. |
| **Helpfulness (H)** | How useful, clear, and relevant the output is to user intent. | Readable, contextual, goal-oriented. |
| **Cost (C)** | The efficiency of the output — time, tokens, or complexity. | Minimum steps, no verbosity. |

✅ *Formula:*

> Prompt Quality Score (PQS) = (F × H) / C
> 

> 💡 High Faithfulness and Helpfulness with low Cost = excellent prompt.
> 

---

### **4️⃣ Extended Evaluation Dimensions**

Beyond FHC, professionals use **7 Key Metrics**:

| **Metric** | **What It Measures** | **Example Evaluation** |
| --- | --- | --- |
| **Clarity** | Is the prompt instruction unambiguous? | “Explain this in simple terms” → vague. |
| **Faithfulness** | Does the output stay true to given data? | Output doesn’t invent facts. |
| **Helpfulness** | Is the output directly useful to user intent? | Actionable and context-relevant. |
| **Conciseness** | Is it free of redundancy? | No unnecessary repetition. |
| **Creativity** | Does it show novel thinking? | Generates unique solutions. |
| **Consistency** | Is it repeatable under similar conditions? | Similar outputs with re-runs. |
| **Cost Efficiency** | Are tokens/time optimized? | Uses less context for same quality. |

✅ *A great prompt balances all seven, but perfection isn’t required — optimization depends on use case.*

---

### **5️⃣ Quantitative vs. Qualitative Evaluation**

| **Approach** | **How It Works** | **Example Use** |
| --- | --- | --- |
| **Quantitative** | Uses scores, token counts, accuracy rates. | “Output matched gold standard 90%.” |
| **Qualitative** | Uses human judgment, rubrics, or peer review. | “Response was clear and empathetic.” |
| **Hybrid** | Combines both for best insight. | Faithfulness (auto), Helpfulness (manual). |

✅ For large projects, combine automated scoring + human QA.

---

### **Part B – Application and Examples**

### **Example 1 – Evaluating a Prompt (FHC Model)**

**Prompt:**

> “Summarize this medical article in 3 bullet points for doctors.”
> 

**Output Evaluation:**

| Metric | Score (1–10) | Notes |
| --- | --- | --- |
| Faithfulness | 9 | Accurate to article facts. |
| Helpfulness | 8 | Clear but a bit verbose. |
| Cost | 7 | Slightly long (120 tokens). |
| ✅ Final PQS = (9 × 8) / 7 ≈ **10.3 (High)** |  |  |

---

### **Example 2 – Comparing Two Prompts**

| **Prompt A** | **Prompt B** |
| --- | --- |
| “Summarize this report briefly.” | “Summarize this report in 3 concise bullet points for executives.” |

| Metric | A | B | Winner |
| --- | --- | --- | --- |
| Faithfulness | 8 | 9 | B |
| Helpfulness | 6 | 9 | B |
| Cost | 9 | 7 | A |
| **PQS** | (8×6)/9=5.3 | (9×9)/7=11.6 | **B** ✅ |

✅ Prompt B wins — clearer audience and structure improved results.

---

### **Example 3 – Adding Creativity Evaluation**

**Prompt:**

> “Write a social media caption for a new smartwatch.”
> 

| Metric | Score | Comment |
| --- | --- | --- |
| Faithfulness | 10 | Correct details. |
| Helpfulness | 8 | Clear product message. |
| Creativity | 9 | Original and engaging. |
| Cost | 7 | 60 tokens, efficient. |

✅ *High creativity boosts engagement, balancing function and flair.*

---

### **Example 4 – Evaluating Ethical Safety**

**Prompt:** “List side effects of a medication.”

**Output:** Mentions real effects + a false one.

| Metric | Score | Comment |
| --- | --- | --- |
| Faithfulness | 6 | Added incorrect info. |
| Safety | 4 | Could mislead users. |
| Helpfulness | 8 | Good clarity. |

✅ Needs rework — accuracy and factual grounding must come first.

---

### **Example 5 – Cost-Efficiency Evaluation**

**Prompt 1:** “Analyze this 10-page report thoroughly.”

**Prompt 2:** “Summarize key insights in under 150 words, focusing on results.”

| Metric | P1 | P2 | Comment |
| --- | --- | --- | --- |
| Tokens | 1500 | 300 | P2 is 5× cheaper. |
| Faithfulness | 9 | 8 | Slight drop. |
| PQS | (9×10)/1500=0.06 | (8×9)/300=0.24 | ✅ P2 wins in efficiency. |

---

### **Part C – Reflection, Quiz & Mini Project**

### **Reflection Prompt**

> Think about one of your best prompts so far.
> 
> 
> How would you score it using the FHC model (Faithfulness, Helpfulness, Cost)?
> 
> Would you adjust it for higher cost-efficiency or better helpfulness?
> 

---

### **Quick Quiz**

| **Q#** | **Question** | **Type** |
| --- | --- | --- |
| 1 | What does FHC stand for? | Short Answer |
| 2 | Define “Faithfulness” in prompt evaluation. | Short Answer |
| 3 | Why is “Cost” an important metric? | Short Answer |
| 4 | What’s the difference between quantitative and qualitative evaluation? | Short Answer |
| 5 | What is the formula for PQS? | Short Answer |

---

### **Answer Key**

| **Q#** | **Answer** | **Explanation** |
| --- | --- | --- |
| 1 | Faithfulness, Helpfulness, Cost. | Core evaluation triad. |
| 2 | The accuracy and factual integrity of model output. | Prevents hallucination. |
| 3 | It measures efficiency in time/tokens. | Optimizes performance. |
| 4 | Quantitative = measurable; Qualitative = judgment-based. | Two lenses of analysis. |
| 5 | PQS = (F × H) / C. | Prompt Quality Score formula. |

---

### **Mini Project – “Evaluate and Optimize”**

> Goal: Pick one of your prompts and evaluate it using the FHC + Extended 7 Metric System.
> 

| **Step** | **Instruction** | **Example** |
| --- | --- | --- |
| 1️⃣ | Choose a prompt you use often. | “Summarize articles for students.” |
| 2️⃣ | Run 3 test outputs. | Collect responses. |
| 3️⃣ | Score each using Faithfulness, Helpfulness, Cost, Creativity. | Table format. |
| 4️⃣ | Average your scores → PQS. | (8×9)/6 = 12. |
| 5️⃣ | Revise prompt for improvement and re-test. | Compare before vs. after. |

✅ *Advanced Tip:* Use a “meta-evaluator” prompt:

> “Act as a prompt evaluator. Rate the following response on Faithfulness, Helpfulness, and Cost.”
> 

---

### **Instructor Notes (Optional)**

| **Criterion** | **Score (1–10)** | **Focus Area** |
| --- | --- | --- |
| Metric Understanding |  | Explains FHC clearly. |
| Application Accuracy |  | Evaluates correctly. |
| Revision Skill |  | Improves based on scores. |
| Reflection Depth |  | Aware of optimization trade-offs. |

---

✅ **Summary Insight**

> Evaluation is the science of prompt engineering.
> 
> 
> It transforms intuition into measurable insight — turning creative prompting into a reproducible craft.
> 
> A true **Prompt Engineer** doesn’t stop at “good enough.”
> 
> They **measure, compare, and optimize** — because quality isn’t accidental, it’s engineered.
>