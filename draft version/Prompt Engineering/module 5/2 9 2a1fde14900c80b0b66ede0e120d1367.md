# 2.9

# ðŸ§© **1.3.2.9 Exam Section â€” Application + Code Challenge**

---

## ðŸ§  **Part A â€“ Overview**

### **Purpose of This Exam**

This final section assesses your ability to:

- Combine **reasoning patterns** (CoT, ReAct, ToT)
- Apply **few-shot techniques** to real problems
- Build **analytical and self-correcting prompts**
- Evaluate outputs using **FHC metrics**

> ðŸ§© Analogy:
> 
> 
> Youâ€™re now the AI architect â€” your job isnâ€™t to *get an answer*, but to *design the reasoning process itself.*
> 

âœ… The exam mirrors real-world prompt engineering tasks â€” reasoning, testing, and refinement.

---

### **Exam Format**

| **Section** | **Focus** | **Evaluation Mode** |
| --- | --- | --- |
| A | Theory Application | Written / Conceptual |
| B | Pattern Identification | Matching / Analysis |
| C | Code Challenge | Coding + Prompt Design |
| D | Reflection | Critical Thinking |
| E | Rubric Evaluation | Instructor Assessment |

---

## ðŸ§© **Part B â€“ Written Application (10 Questions)**

| **Q#** | **Question** | **Expected Competency** |
| --- | --- | --- |
| 1 | Define Few-Shot Prompting and explain its advantage over Zero-Shot. | Concept clarity |
| 2 | Describe how Chain-of-Thought improves logical transparency. | Reasoning awareness |
| 3 | What makes the ReAct pattern essential for AI agents? | Tool-based reasoning |
| 4 | Explain the difference between ToT and Self-Consistency. | Divergent vs convergent reasoning |
| 5 | Why is reflection important in prompt engineering? | Self-correction understanding |
| 6 | How would you apply Critic-Helper to improve a factual summary prompt? | Meta-evaluation |
| 7 | In what cases should you use Majority Voting across multiple outputs? | Reliability evaluation |
| 8 | Describe the FHC model in your own words. | Evaluation metrics mastery |
| 9 | Whatâ€™s the advantage of JSON-based outputs for prompt evaluation? | Structural thinking |
| 10 | How does few-shot prompting simulate training within the context window? | Cognitive emulation |

âœ… *Each question tests conceptual and applied fluency â€” not memorization.*

---

## âš™ï¸ **Part C â€“ Pattern Matching Challenge (Applied Reasoning)**

> Identify the reasoning pattern(s) used or required in each case:
> 

| **Scenario** | **Best Pattern(s)** | **Why** |
| --- | --- | --- |
| 1ï¸âƒ£ Model generates random answers for same input. | Self-Consistency / Majority Voting | Improves reliability. |
| 2ï¸âƒ£ AI produces long irrelevant content. | Question Refinement + Constraints | Tightens scope. |
| 3ï¸âƒ£ You need reasoning + web search integration. | ReAct Pattern | Combines reasoning + action. |
| 4ï¸âƒ£ You want multiple hypothesis generation. | Tree-of-Thought (ToT) | Branching reasoning. |
| 5ï¸âƒ£ AI gives confident but wrong data. | Reflection + Critic-Helper | Self-verification pass. |
| 6ï¸âƒ£ Generate 3 example-based solutions for a dataset. | Few-Shot Prompting | Demonstration-based learning. |
| 7ï¸âƒ£ You want iterative debugging of Python code. | CoT + Reflection | Logical + self-fixing steps. |
| 8ï¸âƒ£ AI explains logic but skips verification. | Cognitive Verifier Pattern | Adds accuracy layer. |
| 9ï¸âƒ£ Combine empathy + structured output. | Persona + Outline Pattern | Balanced voice + format. |
| ðŸ”Ÿ Evaluate responses for factual integrity. | Faithfulness (FHC) Metric | Evaluation consistency. |

âœ… These scenarios assess **pattern fluency** and **diagnostic accuracy**.

---

## ðŸ’» **Part D â€“ Code Challenge: Build a Reasoning-Driven AI Task**

### **Challenge Objective**

Create a **Few-Shot + Chain-of-Thought + Reflection** prompt that performs **data analysis or code reasoning** with reliability and explainability.

---

### **Problem Statement**

> Design a prompt that analyzes a small dataset or code snippet step-by-step, verifies its logic, and outputs the result in JSON format.
> 

**Example Input:**

```
Data: [12, 15, 20, 23, 25, 30]
Task: Calculate average, detect outliers, and explain reasoning.

```

---

### **Step-by-Step Guide**

| **Stage** | **Prompt Component** | **Example Implementation** |
| --- | --- | --- |
| 1ï¸âƒ£ | Instruction | â€œYou are a data analyst. Think step-by-step to calculate insights.â€ |
| 2ï¸âƒ£ | Few-Shot Example | â€œExample: [10, 20, 30] â†’ avg=20, outlier=None, reason: evenly spaced.â€ |
| 3ï¸âƒ£ | Reasoning (CoT) | â€œFirst compute average, then evaluate deviation per point.â€ |
| 4ï¸âƒ£ | Reflection Layer | â€œNow verify your logic. Correct if any step seems inconsistent.â€ |
| 5ï¸âƒ£ | Output Schema | â€œReturn JSON: {average, outliers, reasoning}â€ |

---

### **Expected AI Output**

```json
{
  "average": 20.8,
  "outliers": [],
  "reasoning": "Each value falls within 2 standard deviations, indicating stable data."
}

```

âœ… **Why This Works:**

It merges **Few-Shot (pattern learning)** + **CoT (step-by-step logic)** + **Reflection (error correction)** â€” the trifecta of intelligent prompting.

---

### **Bonus Challenge: ReAct-Enhanced Version**

Add a â€œtoolâ€ action layer â€” e.g., instruct the model to *simulate running a calculation* or *fetch reference data*:

```
Thought: I need to calculate variance.
Action: Compute variance of dataset.
Observation: Variance = 40.2.
Answer: No significant outliers found.

```

âœ… Simulates agent-like reasoning â€” perfect for **advanced engineering tasks**.

---

## ðŸ§© **Part E â€“ Evaluation Rubric**

| **Dimension** | **Points** | **Description** |
| --- | --- | --- |
| Concept Mastery | 20 | Demonstrates understanding of all major reasoning patterns. |
| Application Design | 20 | Uses multi-pattern integration logically. |
| Code Implementation | 20 | Builds working Few-Shot + CoT prompt with structured logic. |
| Reflection & Verification | 20 | Adds self-correction or Critic-Helper layer. |
| Evaluation Insight | 20 | Applies metrics (FHC or custom). |
| **Total** | **100 pts** |  |

âœ… *Instructor/peer review can be conducted using the same rubric or self-assessment.*

---

## ðŸ§© **Part F â€“ Reflection Section**

> Reflect on your own evolution through Module 5:
> 
> 
> 1ï¸âƒ£ Which reasoning technique (CoT, ReAct, ToT, or Reflection) changed how you design prompts?
> 
> 2ï¸âƒ£ How has your understanding of â€œteaching AI with examplesâ€ evolved?
> 
> 3ï¸âƒ£ Describe one improvement youâ€™ll implement in your future prompt workflows.
> 

âœ… Encourage a **200-word written reflection** or **self-review recording** as part of assessment.

---

## ðŸ§¾ **Part G â€“ Instructor Extensions (Optional)**

| **Activity** | **Goal** |
| --- | --- |
| **Peer Pattern Review** | Students swap and critique each otherâ€™s prompt logic. |
| **Prompt Debugging Challenge** | Identify and fix poor reasoning prompts. |
| **Automated Scoring Script** | Use FHC + token cost metrics to auto-score submissions. |

---

## âœ… **Part H â€“ Final Summary Insight**

> Module 5 represents the transition from prompting as input-output to prompting as system design.
> 
> 
> You now understand:
> 
> - How Few-Shot creates demonstration-based intelligence.
> - How CoT and ToT structure AI reasoning.
> - How ReAct and Critic-Helper bring verification and action.
> - And how metrics and reflection sustain continuous improvement.
> 
> ðŸ’¡ *Final Thought:*
> 
> â€œPrompt Engineering isnâ€™t about talking to AI â€” itâ€™s about teaching it how to think.â€
> 

---

âœ… **Outcome:**

Upon completing this exam, learners can **design, test, debug, and evaluate reasoning-driven prompts** suitable for real-world use in **data analysis, automation, and intelligent system design.**