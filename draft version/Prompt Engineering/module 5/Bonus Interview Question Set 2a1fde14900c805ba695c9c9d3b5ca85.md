# Bonus Interview Question Set

# üéì **Module 5 ‚Äì Bonus Interview Question Set (20 Questions)**

*(with Full Answer Key & Rubric)*

---

## üß© **Section A ‚Äì Conceptual Understanding**

**1Ô∏è‚É£** Define **Zero-Shot Prompting** and explain when it‚Äôs most effective.

**2Ô∏è‚É£** What is the main advantage of **Few-Shot Prompting** compared to Zero-Shot?

**3Ô∏è‚É£** Describe the **Chain-of-Thought (CoT)** technique in one sentence.

**4Ô∏è‚É£** What are the key differences between **CoT** and **ReAct** patterns?

**5Ô∏è‚É£** Define **Tree-of-Thought (ToT)** reasoning and explain its benefit.

**6Ô∏è‚É£** What problem does the **Self-Consistency** method solve?

**7Ô∏è‚É£** What is the purpose of **Majority Voting** in multi-agent reasoning?

**8Ô∏è‚É£** Explain how the **Reflection Pattern** improves model outputs.

**9Ô∏è‚É£** What are the roles in a **Critic-Helper** pattern?

**üîü** Why are **Prompt Evaluation Metrics** (like FHC) essential for iterative improvement?

---

## ‚öôÔ∏è **Section B ‚Äì Applied Scenarios**

**11Ô∏è‚É£** You need to teach the model how to write executive summaries with a specific format.

Which prompting method would you use and why?

**12Ô∏è‚É£** The AI gives inconsistent answers for a math problem.

How would **Self-Consistency** improve reliability?

**13Ô∏è‚É£** The model generates code with minor syntax errors.

Which reasoning and reflection combination would best fix this?

**14Ô∏è‚É£** You want the model to retrieve live data, reason about it, and verify conclusions.

Which technique should you use?

**15Ô∏è‚É£** You‚Äôre designing a research assistant that must justify each conclusion with evidence.

How could you combine **ToT + Reflection** for this task?

**16Ô∏è‚É£** In data analysis, the model misinterprets CSV column meanings.

Which prompting pattern would clarify structure and logic?

**17Ô∏è‚É£** The AI gives confident but wrong results in financial analysis.

Which corrective patterns would prevent this?

**18Ô∏è‚É£** You want the model to generate multiple hypotheses before deciding the best one.

Which pattern structure enables that reasoning style?

**19Ô∏è‚É£** You‚Äôre tasked to create a ‚Äúlearning agent‚Äù that critiques and revises its own outputs.

Which two patterns together accomplish this?

**20Ô∏è‚É£** During model evaluation, you want to rank three prompts for faithfulness and efficiency.

Which metric framework applies, and how would you use it?

---

---

# ‚úÖ **Answer Key & Explanations (Rebux Model)**

| **Q#** | **Answer Summary** | **Explanation (Rebux Insight)** |
| --- | --- | --- |
| 1 | Task completion without examples. | Ideal for general or known tasks (translation, sentiment). |
| 2 | Provides few examples to demonstrate structure. | Teaches model through pattern imitation. |
| 3 | A reasoning technique that instructs the AI to think step-by-step. | Enhances logical clarity and correctness. |
| 4 | CoT = internal reasoning only; ReAct = reasoning + external action/observation. | ReAct adds tool integration. |
| 5 | Multi-branch reasoning that explores several potential solutions. | Encourages divergent and reflective thinking. |
| 6 | Reduces randomness by comparing multiple reasoning outputs. | Selects the most consistent result. |
| 7 | Chooses the most common answer across runs/models. | Builds consensus and reliability. |
| 8 | Instructs the model to review and improve its own output. | Adds metacognitive self-checking. |
| 9 | Critic identifies errors; Helper revises them. | Creates internal feedback loop. |
| 10 | FHC (Faithfulness, Helpfulness, Cost) evaluates prompt quality. | Enables data-driven iteration. |
| 11 | Few-Shot Prompting. | Learns from format and tone examples. |
| 12 | Generate multiple reasoning chains, compare, pick consistent output. | Improves reliability by averaging paths. |
| 13 | CoT + Reflection. | First reason logically, then self-correct. |
| 14 | ReAct Pattern. | Combines reasoning with actions like API calls or retrieval. |
| 15 | ToT (multi-branch logic) + Reflection (self-evaluation). | Generates, evaluates, and verifies hypotheses. |
| 16 | Few-Shot + Template Pattern. | Teaches structure through clear data examples. |
| 17 | Reflection + Critic-Helper + FHC. | Detects, fixes, and evaluates logical errors. |
| 18 | Tree-of-Thought (ToT). | Expands reasoning space to explore multiple solutions. |
| 19 | Reflection + Critic-Helper. | Enables iterative improvement and self-assessment. |
| 20 | FHC or extended metric set (FHC + Creativity + Safety). | Quantitatively compare and optimize prompt variants. |

---

# üßÆ **Scoring Rubric (Rebux Evaluation Framework)**

| **Dimension** | **Weight** | **Performance Indicators** |
| --- | --- | --- |
| Concept Understanding | 25 pts | Defines each reasoning technique clearly and correctly. |
| Application Accuracy | 25 pts | Chooses the correct pattern for given scenarios. |
| Integration Skill | 20 pts | Combines multiple reasoning styles effectively. |
| Evaluation Awareness | 15 pts | Applies FHC or extended metrics appropriately. |
| Reflection Depth | 15 pts | Demonstrates meta-level awareness of reasoning quality. |
| **Total** | **100 pts** |  |

---

# üß≠ **Proficiency Scale**

| **Score Range** | **Title** | **Descriptor** |
| --- | --- | --- |
| 90‚Äì100 | üß† **Reasoning Architect** | Expert in designing and evaluating multi-pattern prompts. |
| 75‚Äì89 | ‚öôÔ∏è **Applied Prompt Engineer** | Solid grasp of reasoning and reflection loops. |
| 60‚Äì74 | üß© **Developing Practitioner** | Understands basics but needs practice with integration. |
| < 60 | üå± **Foundational Learner** | Should revisit core reasoning techniques and examples. |

---

# üí¨ **Reflection Assignment**

> In 150‚Äì200 words, reflect on how reasoning patterns (CoT, ReAct, ToT) changed your understanding of prompt design.
> 
> - Which reasoning pattern do you naturally use the most?
> - Which combination (e.g., ReAct + Reflection) feels most effective for complex problems?
> - How would you evaluate your own prompt logic using FHC or a custom rubric?

‚úÖ *Deliverable:* Learners can write or record this reflection as part of their **Prompt Engineering Portfolio** or **Final Evaluation Submission.**

---

# üß† **Summary Insight**

> Module 5 is where you evolve from writing prompts to designing reasoning systems.
> 
> 
> You‚Äôve learned how to teach AI to **think (CoT), act (ReAct), explore (ToT), reflect, and self-correct (Critic-Helper)** ‚Äî all while measuring results with FHC.
> 
> üí° *Rebux Takeaway:*
> 
> ‚ÄúPrompt Engineering isn‚Äôt about single responses ‚Äî it‚Äôs about creating thought architecture.‚Äù
> 

---