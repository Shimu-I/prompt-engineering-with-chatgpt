# 3.6

## 🧩 **1.2.3.6 Self-Reflection & Critique Loops (Ask–Verify–Revise)**

---

### **Part A – Concept Foundations**

### **1️⃣ The Essence of Self-Reflection in Prompt Engineering**

Self-reflection in AI work means **treating every output as feedback**, not as a final product.

It’s the habit of asking:

> “Why did the model answer this way — and how can I guide it better next time?”
> 

This mirrors how great thinkers improve through iteration: they **ask, verify, and revise** until precision emerges.

> 🧠 Analogy:
> 
> 
> Think of AI collaboration like writing with a mirror.
> 
> You see not just what you wrote, but what’s *reflected back* — the model’s interpretation of your clarity (or lack thereof).
> 

---

### **2️⃣ The “Ask–Verify–Revise” Loop**

This is the structured method prompt engineers use to build *intelligent feedback cycles* between themselves and the model.

| **Stage** | **Action** | **Example** |
| --- | --- | --- |
| **Ask** | Generate or inquire. | “Explain reinforcement learning in simple terms.” |
| **Verify** | Critically review the output. | “Did it define key terms correctly? Was it simple enough?” |
| **Revise** | Modify the prompt or request clarifications. | “Simplify further. Use an analogy with a student and teacher.” |

> ⚙️ Repeat until the response meets clarity, correctness, and completeness standards.
> 

---

### **3️⃣ Why Reflection Is Essential for Engineers**

| **Without Reflection** | **With Reflection** |
| --- | --- |
| Blindly trusts AI outputs | Treats outputs as first drafts |
| Repeats errors unknowingly | Learns from system feedback |
| No measurable improvement | Continuous skill growth |
| Static thinking | Meta-cognitive prompting |

✅ Reflection = *self-debugging intelligence* for humans and models alike.

---

### **4️⃣ The “Critique Loop” Pattern**

**Definition:**

A *Critique Loop* is when you instruct the model to **evaluate or score its own previous output** before continuing.

This simulates self-awareness (though it’s pattern-based, not conscious).

> 💬 Example:
> 
> 
> ```
> Task: Write a 3-paragraph summary on climate ethics.
> Next: Critique your summary for clarity, balance, and tone. Suggest 2 improvements.
> Then: Revise your answer using those suggestions.
> 
> ```
> 
> ✅ *Result:* The model becomes both writer and reviewer — building quality through recursive improvement.
> 

---

### **5️⃣ Reflection vs Revision**

| **Stage** | **Focus** | **Engineer’s Role** |
| --- | --- | --- |
| **Reflection (Meta)** | Understanding why the AI behaved a certain way | Ask probing questions |
| **Revision (Action)** | Improving future prompts | Apply what was learned |

> 🔄 Together, they create intelligent iteration — a signature of expert prompting.
> 

---

### **Part B – Application and Examples**

### **Example 1 — The Simple Reflection Prompt**

Prompt:

> “Explain blockchain.”
> 

**Output:** (Simplified, short explanation.)

Now reflect:

> “Now review your previous answer. Was it complete? If not, rewrite it to include an example of real-world blockchain usage.”
> 

✅ *Result:* More complete and contextual explanation.

---

### **Example 2 — The 3-Step Self-Critique Chain**

**Prompt Sequence:**

1️⃣ *Ask:* “Write a 2-paragraph summary of AI in education.”

2️⃣ *Verify:* “Critique your summary for structure, accuracy, and clarity.”

3️⃣ *Revise:* “Improve the summary using your critique.”

✅ *Outcome:* Second version is usually 30–50% clearer and more coherent.

> 🧩 Insight: The model “learns” through structured prompting — not memory, but guided reasoning.
> 

---

### **Example 3 — Peer Review Simulation**

Prompt:

> “Act as a reviewer. Evaluate this prompt:
> 
> 
> ‘Write an engaging article about clean energy.’
> 
> Provide a score (1–10) and suggest 3 prompt improvements.”
> 

✅ *AI Output Example:*

- Score: 6/10
- Improvements:
    
    1️⃣ Define target audience.
    
    2️⃣ Specify tone and style.
    
    3️⃣ Limit to 500 words.
    

Revised Prompt:

> “Act as an energy journalist. Write a 500-word article on clean energy for young professionals, using an optimistic tone.”
> 

✅ *Result:* Output dramatically improves.

---

### **Example 4 — Critique-Helper Pattern**

**Prompt:**

> “Act as a Critique-Helper. For the text below, point out what works well and what could be improved, without rewriting it.”
> 

✅ *Effect:*

The AI separates *evaluation* from *execution*, promoting focused, layered reasoning — much like a human editor.

---

### **Example 5 — Self-Reflection in Code Tasks**

Prompt:

> “Write Python code to calculate factorial recursively.”
> 
> 
> Then:
> 
> “Explain how your code works step-by-step, then identify any possible optimization.”
> 

✅ *Output:* Adds reasoning trace and potential improvements — ideal for education or debugging workflows.

---

### **Part C – Reflection, Quiz, and Mini Project**

### **Reflection Prompt**

> Think about your last five AI prompts.
> 
> 
> Did you ever *ask the model to verify or critique* its own work?
> 
> What might change if you made that a habit?
> 

---

### **Quick Quiz**

| **Q#** | **Question** | **Type** |
| --- | --- | --- |
| 1 | What is the Ask–Verify–Revise loop? | Short Answer |
| 2 | How does a critique loop improve AI output? | Short Answer |
| 3 | What’s the difference between reflection and revision? | Short Answer |
| 4 | Why should prompts include meta-evaluation steps? | Short Answer |
| 5 | How can self-reflection improve prompt engineers themselves? | Short Answer |

---

### **Answer Key**

| **Q#** | **Answer** | **Explanation** |
| --- | --- | --- |
| 1 | Structured method to generate, evaluate, and refine outputs iteratively. | Self-feedback cycle. |
| 2 | Forces model to analyze and fix its own weaknesses. | Recursive improvement. |
| 3 | Reflection = insight; Revision = action. | Distinct stages. |
| 4 | Ensures quality control and accountability in automation. | Embedded QA. |
| 5 | Encourages critical thinking and meta-awareness in AI design. | Cognitive growth. |

---

### **Mini Project: “Critique the Critic”**

> Goal: Practice building a self-improving AI dialogue.
> 

| **Step** | **Instruction** | **Example** |
| --- | --- | --- |
| 1️⃣ | Pick a short writing or coding prompt. | “Write a motivational paragraph for students.” |
| 2️⃣ | Add a critique instruction. | “Now review your output for tone and clarity.” |
| 3️⃣ | Add a revision step. | “Rewrite the paragraph using your critique.” |
| 4️⃣ | Compare versions 1 and 2. | Identify differences in tone or logic. |
| 5️⃣ | Write a short reflection (100–150 words) on what improved and why. | “Adding critique improved emotional resonance.” |

✅ *Extension:* Ask AI to score each version (1–10) and justify the score — creating measurable self-assessment data.

---

### **Instructor Notes (Optional)**

| **Criterion** | **Score (1–10)** | **Focus Area** |
| --- | --- | --- |
| Loop Structure |  | Proper execution of Ask–Verify–Revise |
| Output Quality |  | Clear evidence of improvement |
| Reflection Depth |  | Learner awareness and reasoning |
| Critique Use |  | Balanced evaluation vs rewrite |

---

✅ **Summary Insight:**

> Self-reflection and critique loops are the heart of adaptive intelligence.
> 
> 
> By embedding the **Ask–Verify–Revise** method in your prompt design, you teach both yourself and the model to evolve with every cycle.
> 
> Great prompt engineers don’t just craft commands — they craft **feedback ecosystems.**
>