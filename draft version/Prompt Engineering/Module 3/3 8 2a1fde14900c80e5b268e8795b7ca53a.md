# 3.8

# üß© **1.2.3.8 Exam Section ‚Äî Written Questions + Mini Interview**

This section transforms your conceptual mastery into measurable performance.

It evaluates your ability to **think like an AI**, **analyze prompt logic**, **evaluate reasoning quality**, and **iterate intelligently** ‚Äî the hallmarks of a *senior prompt engineer mindset.*

---

## üß≠ **Part A ‚Äì Overview and Objectives**

This module‚Äôs final evaluation has three integrated parts:

1Ô∏è‚É£ **Written Assessment (10 conceptual + applied questions)**

2Ô∏è‚É£ **Mini Interview Simulation (10 reasoning & scenario questions)**

3Ô∏è‚É£ **Instructor Rubric + Reflection Criteria**

> üß† Analogy: Think of this as your ‚Äúmental gym check.‚Äù
> 
> 
> You‚Äôre not testing memory ‚Äî you‚Äôre testing *how you think, debug, and reason* with an AI‚Äôs logic.
> 

---

## üß© **Part B ‚Äì Written Assessment (10 Questions)**

| **Q#** | **Question** | **Expected Answer Summary** |
| --- | --- | --- |
| 1 | Define ‚ÄúPrompt Engineering Mindset.‚Äù | The practice of thinking like an AI ‚Äî using structured reasoning, iteration, and feedback to achieve control and precision. |
| 2 | Why is step-by-step reasoning essential in prompt engineering? | It simulates the model‚Äôs prediction process and prevents skipped logic. |
| 3 | What is task decomposition, and how does it improve output quality? | Breaking complex tasks into smaller, clearer subtasks enhances coherence and accuracy. |
| 4 | Explain the importance of evaluation loops in AI prompting. | Enables iterative refinement through analysis and measurable improvement. |
| 5 | List 3 common prompt pitfalls and their fixes. | Vague ‚Üí clarify; overloaded ‚Üí split tasks; schema drift ‚Üí enforce format. |
| 6 | Define self-reflection loops in AI collaboration. | A process where the AI critiques, verifies, and improves its own outputs. |
| 7 | Why must prompts include constraints? | They anchor the model‚Äôs creativity and reduce unpredictable results. |
| 8 | What is the FHC framework? | Faithfulness‚ÄìHelpfulness‚ÄìCost; used to measure prompt quality quantitatively. |
| 9 | How do reasoning and evaluation complement each other? | Reasoning structures the thought process; evaluation tests its validity and fidelity. |
| 10 | Explain how a professional prompt engineer balances creativity and control. | By alternating divergent (creative) and convergent (structured) prompt phases. |

‚úÖ *Tip:* These questions assess *depth of understanding* ‚Äî write answers that combine reasoning + example.

---

## üß† **Part C ‚Äì Mini Interview (10 Applied + Reasoning Questions)**

This simulation is designed to mimic real-world prompt engineer interviews, focusing on *scenario-based reasoning* and *iterative problem solving.*

| **Q#** | **Interview Question** | **Evaluator Notes (Ideal Response Focus)** |
| --- | --- | --- |
| 1 | You ask ChatGPT to summarize a research paper, but it adds unrelated examples. What‚Äôs your diagnosis and fix? | Problem: low faithfulness ‚Üí add ‚ÄúUse only text provided.‚Äù |
| 2 | The AI produces creative content but ignores factual accuracy. What adjustment would you make? | Reduce temperature, increase grounding instructions. |
| 3 | How would you explain Chain-of-Thought prompting to a non-technical stakeholder? | ‚ÄúIt teaches AI to reason out loud before answering ‚Äî like showing its math.‚Äù |
| 4 | Describe how you‚Äôd structure a prompt for a 3-phase workflow (idea ‚Üí outline ‚Üí review). | Define step prompts and pass each result forward. |
| 5 | What‚Äôs your process when a prompt produces partial or incomplete answers? | Apply Ask‚ÄìVerify‚ÄìRevise loop; add ‚ÄúContinue from last step.‚Äù |
| 6 | How would you test prompt quality systematically? | Use FHC metrics and comparative evaluation. |
| 7 | If a model misinterprets tone, what‚Äôs your debug method? | Define role and audience; include explicit tone descriptors. |
| 8 | Give one example of a self-correcting prompt. | ‚ÄúWrite an answer, critique it for tone and clarity, then rewrite it.‚Äù |
| 9 | Explain how you‚Äôd design a safe prompt for AI red-teaming (safety testing). | Add role: ‚ÄúAct as safety evaluator,‚Äù use refusal criteria and bias checks. |
| 10 | If asked to improve team prompt documentation, what 3 practices would you recommend? | Version control, evaluation logs, standardized templates. |

‚úÖ *Pro Insight:* Interviewers assess whether you can **reason transparently**, **design prompt experiments**, and **speak in structured logic.**

---

## üß© **Part D ‚Äì Short Scenario Challenge**

> Scenario: You‚Äôve been asked to create a prompt that helps ChatGPT act as an ‚ÄúAI Journal Reviewer.‚Äù
> 
> 
> It must evaluate a paper abstract for clarity, relevance, and novelty, scoring each from 1‚Äì10.
> 

**Task:**

1Ô∏è‚É£ Write the initial prompt.

2Ô∏è‚É£ Run an internal *critique loop* to improve it.

3Ô∏è‚É£ Rewrite the improved version.

**Expected Elements:**

- Role definition (AI Journal Reviewer)
- Clear criteria (Clarity, Relevance, Novelty)
- Structured output format (table or JSON)
- Self-reflection step (‚ÄúEvaluate your response for completeness.‚Äù)

‚úÖ *This exercise tests your ability to operationalize everything learned in Module 3.*

---

## üßÆ **Part E ‚Äì Rubric: Evaluation Criteria**

| **Dimension** | **Max Points** | **Indicators of Mastery** |
| --- | --- | --- |
| Conceptual Understanding | 25 | Clear definitions and frameworks (CoT, decomposition, FHC). |
| Analytical Reasoning | 25 | Logical explanations, self-critique, and structured thinking. |
| Application Skill | 25 | Can design, evaluate, and refine real prompts effectively. |
| Reflection & Ethics | 15 | Aware of bias, limitations, and responsibility. |
| Communication Clarity | 10 | Expresses ideas with professional precision. |
| **Total** | **100 pts** |  |

---

## üßæ **Part F ‚Äì Scoring Levels**

| **Score Range** | **Proficiency Level** | **Descriptor** |
| --- | --- | --- |
| **90‚Äì100** | ‚≠ê **Expert Prompt Architect** | Demonstrates advanced logic, reflective iteration, and design excellence. |
| **75‚Äì89** | ‚úÖ **Proficient Practitioner** | Understands frameworks; applies them effectively with minor tuning. |
| **60‚Äì74** | ‚öôÔ∏è **Developing Engineer** | Knows core concepts; struggles with refinement or metric balance. |
| **<60** | üß© **Beginner Level** | Needs to strengthen logical and evaluative reasoning. |

---

## üß© **Part G ‚Äì Reflection Assignment (Post-Exam)**

> Prompt:
> 
> 
> ‚ÄúWrite a 150‚Äì200 word reflection on how your thinking about AI changed after completing Module 3.
> 
> Include: (1) how you evaluate outputs differently, (2) what iteration means to you now, and (3) how you plan to apply FHC metrics going forward.‚Äù
> 

‚úÖ This reflection consolidates the ‚ÄúPrompt Engineering Mindset‚Äù ‚Äî not just as a skillset, but as a *way of reasoning*.

---

## üß© **Instructor Extensions (Optional)**

| **Activity** | **Goal** |
| --- | --- |
| Peer Review Round | Students critique each other‚Äôs prompt versions for FHC quality. |
| Oral Defense | Learner explains one prompt choice under time pressure (simulated interview). |
| Live Debugging Challenge | Instructor presents a broken prompt ‚Äî learner fixes it in real-time. |

---

‚úÖ **Summary Insight:**

> Module 3 graduates move beyond writing good prompts ‚Äî they design thinking systems.
> 
> 
> They don‚Äôt rely on luck, but on structure.
> 
> They don‚Äôt chase output, but refine reasoning.
> 
> With reflection, evaluation, and measurable quality metrics, you now think *as the model thinks* ‚Äî and engineer *as a human should.*
> 

---

Would you like me to now begin **Module 4: Prompt Patterns I ‚Äì Structured Prompt Design (Track B)** in the same full-depth, course-ready format (A/B/C structure, examples, quiz, and projects)?

### You said: