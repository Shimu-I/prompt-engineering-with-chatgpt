# Bonus Interview Question Set

# ğŸ§  **Module 3 â€” Bonus Interview Question Set**

### *Topic: â€œPrompt Engineering Mindsetâ€*

---

## ğŸ“ **A. Conceptual & Analytical Questions (Q1â€“Q10)**

| **Q#** | **Interview Question** |
| --- | --- |
| 1 | Define â€œPrompt Engineering Mindsetâ€ and explain why it matters in AI collaboration. |
| 2 | What is the main difference between a *prompt* and a *prompting process*? |
| 3 | Describe how â€œthinking like an AIâ€ changes the way a human structures prompts. |
| 4 | What are the key elements of â€œstep-by-step reasoning,â€ and why are they essential for clarity? |
| 5 | How does *task decomposition* reduce AI hallucinations and improve coherence? |
| 6 | Whatâ€™s the relationship between *reasoning* and *faithfulness* in prompt quality? |
| 7 | How can â€œevaluation and iterationâ€ transform a mediocre prompt into an expert-level one? |
| 8 | List three common prompt pitfalls and the corresponding fixes. |
| 9 | Why are critique loops (â€œAskâ€“Verifyâ€“Reviseâ€) considered a form of meta-learning? |
| 10 | How do faithfulness, helpfulness, and cost (FHC) interact when optimizing a prompt? |

---

## âš™ï¸ **B. Practical & Scenario Questions (Q11â€“Q20)**

| **Q#** | **Interview Question** |
| --- | --- |
| 11 | You ask ChatGPT for a 500-word analysis, but it returns only 200 words. What would you check or modify first? |
| 12 | A prompt consistently produces factual errors. Which metric does this violate and how can you fix it? |
| 13 | You give a complex instruction and the output becomes generic. Which decomposition pattern could fix this? |
| 14 | What is schema drift, and whatâ€™s a clear strategy to prevent it? |
| 15 | How does a *self-critique loop* differ from a *user feedback loop* in prompt evaluation? |
| 16 | In what situations would you prioritize *helpfulness* over *faithfulness*? Give one example. |
| 17 | Explain how you would debug a prompt thatâ€™s producing inconsistent tone across versions. |
| 18 | How can a prompt engineer quantify â€œiteration successâ€? |
| 19 | Why is prompt reflection a necessary step in scaling AI workflows? |
| 20 | Describe how you would train a junior prompt engineer to evaluate their own prompts using FHC. |

---

## ğŸ§¾ **C. Answer Key & Explanations**

| **Q#** | **Expected Answer Summary** | **Explanation** |
| --- | --- | --- |
| 1 | A mindset of structured reasoning, iteration, and evaluation when designing prompts. | Enables consistent, reliable AI collaboration. |
| 2 | A prompt is a single instruction; the prompting process involves iterative refinement. | Focus on process, not one-shot output. |
| 3 | You design instructions as linear reasoning paths, not assumptions. | Models need explicit logic to follow. |
| 4 | Breaking problems into logical steps prevents missing details and enhances transparency. | Mirrors the modelâ€™s token-by-token process. |
| 5 | It limits scope and gives context per subtask, preventing confusion. | Each piece is processed independently and logically. |
| 6 | Faithfulness = factual grounding; reasoning ensures truth follows logic. | Faithfulness depends on structured reasoning. |
| 7 | Iteration allows testing and controlled refinement, leading to clarity and precision. | Turns intuition into data-backed improvement. |
| 8 | Vague â†’ add structure; Overstuffed â†’ split tasks; Schema drift â†’ define format strictly. | Each fix restores focus and consistency. |
| 9 | Because the AI â€œlearnsâ€ to analyze its reasoning patterns via reflection. | Builds adaptive intelligence without retraining. |
| 10 | Improving one often impacts the others; balance yields optimal results. | Over-detail raises cost but may boost faithfulness. |
| 11 | Check word-limit clarity; add â€œEnsure at least 500 wordsâ€ or use â€œexpand further.â€ | Ambiguity in length constraint. |
| 12 | Violates faithfulness; fix by grounding in given data or asking for citations. | Encourages fact alignment. |
| 13 | Use sequential or hierarchical decomposition (outline â†’ expand â†’ refine). | Improves context depth. |
| 14 | Schema drift = AI output deviates from structured format (e.g., JSON). | Fix with explicit schema + â€œno extra textâ€ rule. |
| 15 | Self-critique = AI audits its own output; user feedback = human correction. | Builds AIâ€™s internal reflection cycle. |
| 16 | When user needs insight, not fact â€” e.g., brainstorming ideas. | Prioritize creative helpfulness. |
| 17 | Add explicit tone role (â€œAct as a friendly educatorâ€) and examples. | Reinforces stylistic control. |
| 18 | Use scoring logs or FHC metrics (track % improvement per iteration). | Quantifies refinement effectiveness. |
| 19 | Reflection ensures consistency, scalability, and quality assurance. | Prevents blind repetition of errors. |
| 20 | Teach them to rate each output on Faithfulness, Helpfulness, and Cost (1â€“10). | Builds metric literacy and evaluative habits. |

---

## ğŸ§® **D. Rubric for Evaluation (Instructor/Peer Use)**

| **Dimension** | **Points** | **Indicators of Mastery** |
| --- | --- | --- |
| **Conceptual Understanding** | 25 | Accurately defines and explains frameworks (CoT, FHC, Decomposition). |
| **Analytical Reasoning** | 25 | Demonstrates logical problem-solving and reflective evaluation. |
| **Application Skill** | 25 | Provides real-world, structured prompt examples. |
| **Communication Clarity** | 15 | Uses precise, articulate explanations. |
| **Ethical Awareness** | 10 | Shows understanding of responsible, transparent AI design. |
| **Total** | **100 pts** |  |

---

## ğŸ§¾ **E. Scoring Scale**

| **Score Range** | **Proficiency Level** | **Descriptor** |
| --- | --- | --- |
| **90â€“100** | â­ Expert Prompt Architect | Thinks structurally, iteratively, and analytically. |
| **75â€“89** | âœ… Proficient Practitioner | Strong reasoning and prompt debugging skills. |
| **60â€“74** | âš™ï¸ Developing Engineer | Understands concepts but needs more iteration practice. |
| **<60** | ğŸ§© Beginner | Needs foundational reinforcement in reasoning and evaluation. |

---

## ğŸ§  **F. Reflection Prompts for Learners**

> 1ï¸âƒ£ Which metric (faithfulness, helpfulness, or cost) do you naturally prioritize when prompting?
> 
> 
> 2ï¸âƒ£ How often do you reflect on *why* a prompt worked, not just *that* it worked?
> 
> 3ï¸âƒ£ Whatâ€™s one mindset shift you experienced after learning the Askâ€“Verifyâ€“Revise loop?
> 

Learners should respond in 150â€“200 words as part of post-exam journaling.

---

âœ… **Summary Insight:**

> This interview set moves beyond theory â€” it tests prompt literacy, reasoning maturity, and evaluative precision.
> 
> 
> A true *Prompt Engineer* doesnâ€™t just ask better questions â€” they *engineer thinking patterns* that make AI reasoning structured, reliable, and human-aligned.
> 

---