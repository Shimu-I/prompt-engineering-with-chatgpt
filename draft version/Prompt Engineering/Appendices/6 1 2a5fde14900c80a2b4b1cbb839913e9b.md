# 6.1

## **1.6.1 Appendix A: Prompt Engineering Glossary**

---

### **Part A – Concept Foundations**

A prompt engineer must not only *use* techniques but also *understand* their theoretical roots.

This glossary turns technical jargon into intuitive, human-readable knowledge.

> 🧠 Analogy:
> 
> 
> Think of this glossary as your “AI phrasebook.”
> 
> You’re not memorizing terms — you’re learning the *language of intelligence design.*
> 

---

### **Part B – Core Terms and Expanded Definitions**

| **Term** | **Definition** | **Example / Analogy** |
| --- | --- | --- |
| **ToT (Tree-of-Thoughts)** | A reasoning approach where AI explores multiple solution paths and selects the most logical outcome. | Like a detective considering multiple clues before solving a case. |
| **Self-Consistency** | A method where multiple reasoning chains are generated, and the majority answer is selected. | Like voting among several experts to pick the best consensus. |
| **Agents** | Independent AI entities (personas) that perform specialized tasks and communicate to complete a workflow. | Like departments in a company — each with a defined role. |
| **RAG (Retrieval-Augmented Generation)** | AI retrieves real data from external sources before generating answers, improving accuracy and grounding. | Like a journalist who checks the facts before writing. |
| **Schema Drift** | When output format slowly deviates from its intended structure over multiple runs or iterations. | Like a student’s handwriting getting messier during a long exam. |
| **Red-Teaming** | Stress-testing an AI system to find vulnerabilities, biases, or unsafe behaviors. | Like ethical hacking for AI prompts. |
| **Bias Mitigation** | Strategies used to detect and correct prejudiced or unbalanced outputs. | Balancing the scale so all perspectives are fairly represented. |
| **Hallucination** | When an AI confidently generates incorrect or fabricated information. | Like a person recalling a vivid but false memory. |
| **Faithfulness** | The degree to which an AI’s response aligns with its given data or context. | A faithful summary stays true to its source. |
| **Prompt Guardrails** | Boundaries that control AI behavior through rules, tone, and refusal logic. | Like seatbelts that prevent unsafe AI responses. |
| **LLM Context Window** | The total memory or token space the AI can “see” at once during a conversation. | Like the visible area of a whiteboard before it gets erased. |
| **Output Schema** | A defined format for the AI’s responses (e.g., JSON, table, Markdown). | Ensures consistency, like a form template. |
| **Prompt Drift** | When repetitive prompting changes the model’s tone or interpretation over time. | Similar to a conversation gradually going off-topic. |
| **CoT (Chain-of-Thought)** | Step-by-step reasoning that encourages logical explanation before an answer. | Like showing your math work before solving. |
| **ReAct (Reason + Act)** | Combines reasoning and action — the AI thinks and then executes tasks sequentially. | Like thinking aloud before taking action. |
| **Critic-Helper Pattern** | Dual-agent setup: one AI generates content, another critiques or improves it. | Like peer review between professionals. |
| **Reflection Loop** | A process where AI reviews and improves its own output before finalizing. | Like proofreading your essay before submission. |
| **Hallucination Guard** | Prompts or systems that verify information before final output. | A fact-checker built into the reasoning pipeline. |
| **Safety Layer** | The final filter that blocks harmful or policy-violating outputs. | Like airport security before boarding. |

---

### **Part C – Reflection & Quiz**

---

### **Reflection Prompt**

> Which 3 glossary terms do you find most applicable to your daily work or study?
> 
> 
> How might you use ToT, RAG, or Reflection Loops in your own AI workflows?
> 

---

### **Quick Quiz**

| **Q#** | **Question** |
| --- | --- |
| 1 | What does RAG stand for, and what is its purpose? |
| 2 | How does self-consistency improve AI reliability? |
| 3 | Define schema drift in simple terms. |
| 4 | Why is a safety layer important in AI systems? |
| 5 | What’s the difference between CoT and ReAct? |

---

### **Answer Key (Rebux Format)**

| **Q#** | **Answer** | **Explanation (Rebux)** |
| --- | --- | --- |
| 1 | Retrieval-Augmented Generation — enhances factual accuracy by fetching real data. | Prevents hallucinations and adds evidence. |
| 2 | It compares multiple reasoning paths to find the most consistent result. | Improves dependability in complex logic. |
| 3 | Gradual deviation from a defined output structure. | Fix with schema validation or reminders. |
| 4 | Prevents unsafe or policy-breaking outputs. | Acts as the “ethical firewall.” |
| 5 | CoT = step reasoning; ReAct = reasoning + task execution. | ReAct is more interactive and applied. |

---

✅ **Summary Insight**

> A prompt engineer doesn’t just write prompts — they design reasoning environments.
> 
> 
> Knowing these terms gives you the language to build, debug, and explain AI logic.
> 

💡 *Prompt Engineering Principle:*

> “Understanding terminology turns prompting into engineering.”
>