# 6.6

# 🧩 **1.6.6 Appendix F: Evaluation Rubrics & Peer Review Checklist**

---

## **Part A – Concept Foundations**

---

### **1️⃣ Why Evaluation Matters**

Prompt engineering is both an *art* and a *science.*

To improve systematically, we must **measure** what makes a prompt effective — not just admire the result.

> 🧠 Analogy:
> 
> 
> Think of prompts like musical compositions.
> 
> You can’t improve harmony without sheet music — and evaluation rubrics are that sheet music for your AI responses.
> 

✅ **Definition:**

> Prompt Evaluation:
> 
> 
> The structured process of rating AI responses across qualitative and quantitative metrics — accuracy, clarity, relevance, tone, cost, and safety — to ensure consistency, fairness, and improvement.
> 

---

### **2️⃣ The Five Core Evaluation Dimensions**

| **Dimension** | **Definition** | **Key Question** |
| --- | --- | --- |
| **Accuracy** | Are the facts correct? | “Is it true?” |
| **Faithfulness** | Does it align with the given input/context? | “Is it grounded?” |
| **Helpfulness** | Does it solve the user’s need? | “Is it useful?” |
| **Clarity & Style** | Is it easy to read and understand? | “Is it well written?” |
| **Safety** | Does it avoid bias or harmful content? | “Is it responsible?” |

✅ *Rule:* Never grade a prompt by “feel” — grade by criteria.

---

### **3️⃣ The “GRADE Framework” for Prompt Review**

| **Letter** | **Meaning** | **Purpose** |
| --- | --- | --- |
| **G – Goal Alignment** | Does it fulfill the user’s intent? | Evaluate success vs. instructions. |
| **R – Reasoning Quality** | Logical flow, coherence, depth. | Identify weak logic or leaps. |
| **A – Accuracy** | Fact-checking, verifiability. | Prevent misinformation. |
| **D – Delivery** | Tone, clarity, formatting. | Evaluate professionalism. |
| **E – Ethics** | Bias, fairness, policy compliance. | Maintain responsible output. |

✅ *GRADE = a holistic evaluator for prompt performance.*

---

## **Part B – Application and Rubric Tables**

---

### **1️⃣ Evaluation Rubric (Instructor / Self-Assessment)**

| **Criterion** | **Weight** | **Score (1–10)** | **Indicators** |
| --- | --- | --- | --- |
| **Accuracy** | 25% |  | Factually correct; cites or validates data. |
| **Faithfulness** | 20% |  | Stays consistent with input/context. |
| **Helpfulness** | 20% |  | Solves user task completely and efficiently. |
| **Clarity & Tone** | 15% |  | Professional, readable, and well-formatted. |
| **Safety & Ethics** | 10% |  | Avoids bias or harmful implications. |
| **Efficiency (Cost / Token Use)** | 10% |  | Achieves goal with minimal token waste. |

✅ *Total Score = Weighted Sum (100%)*

*Example Interpretation:*

- **90–100** → Excellent: Production-grade prompt.
- **75–89** → Good: Minor tuning needed.
- **60–74** → Fair: Needs structural improvement.
- **Below 60** → Poor: Redesign required.

---

### **2️⃣ Peer Review Checklist**

| **Checklist Item** | **Yes / No / Notes** |
| --- | --- |
| The prompt includes a clear role or context. |  |
| The output matches the requested tone/style. |  |
| The AI reasoning steps are explicit or verifiable. |  |
| No hallucinated or unverifiable claims appear. |  |
| Ethical standards (SAFE framework) are respected. |  |
| Output formatting follows expected schema. |  |
| Tokens are efficiently used (no redundancy). |  |
| Reviewer suggests one actionable improvement. |  |

✅ *Purpose:* Enables objective, structured feedback between peers.

---

### **3️⃣ Comparative Evaluation Example**

**Prompt A vs Prompt B (Same Task)**

| **Criterion** | **Prompt A** | **Prompt B** | **Better Version** | **Reason** |
| --- | --- | --- | --- | --- |
| Accuracy | 9 | 8 | A | Cites real data |
| Faithfulness | 8 | 9 | B | Follows instructions closely |
| Helpfulness | 7 | 10 | B | More complete answer |
| Clarity | 10 | 8 | A | Cleaner formatting |
| Safety | 9 | 9 | – | Equal compliance |
| **Winner** |  |  | **Prompt B** |  |

✅ Use side-by-side comparisons for iteration testing and A/B optimization.

---

### **4️⃣ Example Evaluation Rubric for Creative Tasks**

| **Criterion** | **Definition** | **Max Score** |
| --- | --- | --- |
| Originality | Novel ideas or creative framing | 10 |
| Emotional Impact | Evokes engagement or response | 10 |
| Structural Coherence | Logical beginning, middle, end | 10 |
| Relevance | Fits prompt purpose or audience | 10 |
| Clarity & Flow | Smooth narrative and transitions | 10 |

✅ *Use for storytelling, design, or content-generation prompts.*

---

### **5️⃣ Bias & Fairness Sub-Rubric**

| **Bias Category** | **Description** | **Score (1–5)** |
| --- | --- | --- |
| Gender / Identity | Inclusive and neutral wording |  |
| Cultural | Avoids stereotypes or Western bias |  |
| Political / Ideological | Balanced representation |  |
| Accessibility | Uses inclusive language |  |
| Transparency | Clearly identifies limitations |  |

✅ *Integrate with SAFE or ACHIEVE frameworks for policy compliance.*

---

## **Part C – Reflection, Quiz & Practice**

---

### **Reflection Prompt**

> Choose one of your own prompts and rate it using the GRADE Framework.
> 
> 
> How did it score on Goal Alignment, Reasoning, Accuracy, Delivery, and Ethics?
> 
> What’s one area where your prompt could improve its “faithfulness” or “efficiency”?
> 

---

### **Quick Quiz**

| **Q#** | **Question** | **Type** |
| --- | --- | --- |
| 1 | What does GRADE stand for? | Short Answer |
| 2 | Which five dimensions form the core evaluation model? | Short Answer |
| 3 | Why is peer review valuable in prompt design? | Short Answer |
| 4 | What’s the difference between accuracy and faithfulness? | Short Answer |
| 5 | Name one bias type assessed in the fairness rubric. | Short Answer |

---

### **Answer Key (Rebux Format)**

| **Q#** | **Answer** | **Explanation (Rebux)** |
| --- | --- | --- |
| 1 | Goal Alignment, Reasoning, Accuracy, Delivery, Ethics. | Comprehensive review structure. |
| 2 | Accuracy, Faithfulness, Helpfulness, Clarity, Safety. | Core LLM output evaluation metrics. |
| 3 | Encourages diverse feedback and unbiased improvement. | Collective quality assurance. |
| 4 | Accuracy = factual correctness; Faithfulness = contextual consistency. | Prevents truth/context confusion. |
| 5 | Gender, cultural, political, or accessibility bias. | Covers fairness dimensions. |

---

### **Mini Project – “Prompt Review Board”**

> Goal: Create a small team or peer group to evaluate and improve each other’s prompts.
> 

| **Step** | **Instruction** | **Example** |
| --- | --- | --- |
| 1️⃣ | Form a 2–3 person review group. | Peers or classmates. |
| 2️⃣ | Select 3 prompts per person. | Cover different use cases. |
| 3️⃣ | Apply the Evaluation Rubric + GRADE Framework. | Score and comment. |
| 4️⃣ | Compile findings into a shared dashboard. | Google Sheets or Notion. |
| 5️⃣ | Redesign low-scoring prompts and resubmit. | Version control your results. |

✅ *Advanced Option:* Conduct monthly “Prompt Improvement Sprints” for ongoing refinement.

---

### **Instructor Rubric (Optional)**

| **Criterion** | **Score (1–10)** | **Focus Area** |
| --- | --- | --- |
| Framework Application |  | Uses GRADE or equivalent system |
| Objectivity |  | Avoids personal bias in evaluation |
| Feedback Depth |  | Constructive, specific improvement notes |
| Documentation |  | Proper scoring and version history |
| Reflection |  | Learner self-awareness shown |

---

✅ **Summary Insight**

> Evaluation is the mirror that reveals a prompt’s true quality.
> 
> 
> It’s not about judgment — it’s about calibration, consistency, and learning through evidence.
> 
> 💡 *Prompt Engineering Principle:*
> 
> “If you can measure it, you can master it.”
>